\documentclass{article}
\usepackage[left=2cm, right=2cm, top=2cm]{geometry}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PACKAGES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{minted}                     % Code
\usepackage{graphicx}                   % PNGs
\usepackage{algorithm}
\usepackage{algpseudocode}              % Algorithms
\usepackage{amsmath}                    % Rightarrow
\usepackage{hyperref}                   % Hyperlinks
\hypersetup{
    colorlinks,
    linkcolor=black
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagenumbering{gobble}

\title{\textbf{Homework #1}}
\author{MacMillan, Kyle}
\date{September 28, 2018}


\begin{document}

\maketitle
\addcontentsline{toc}{section}{Title}

\newpage
\pagenumbering{roman}   % Set TOC page numbering to lowercase roman numerals
\tableofcontents
\addcontentsline{toc}{section}{Table of Contents}

\newpage
\listoffigures
\addcontentsline{toc}{section}{List of Figures}

\listofalgorithms
\addcontentsline{toc}{section}{List of Algorithms}

\newpage
\pagenumbering{arabic}  % Set content page numbering to arabic numerals
% Setup Hyperlinks for the rest of the document
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=blue,
    urlcolor=blue
}

\section{Problem 1}}
\setcounter{page}{1} % Set the page counter to 3
\textit{What are the identity values for the operators}: $\&\&,\ ||,\ |,\ ^\wedge $?

\begin{itemize}
    \begin{tabular}{@{}ll}
    \item $\&\&$ &: 1\\
    \item $||$ &: 0\\
    \item $|$ &: 0\\
    \item $^\wedge$ &: 0\\
    \end{tabular}
\end{itemize}



\section{Problem 2}
\textit{Suppose OpenMP did not have the reduction clause. Show how to implement an efficient parallel reduction by adding a private variable and using the critical pragma.}
\inputminted{c++}{problem2.cpp}

\begin{figure}[h]
    \centering
    \begin{minipage}{0.7\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Problem2debug}
        \caption[Example Debug Output]{Example debug output.}
        \label{fig:p2debug}
    \end{minipage}\hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{Problem2_60}
        \caption[Timed Reduction Methods]{Better performance without reduction.}
        \label{fig:p2}
    \end{minipage}
\end{figure}

As can be seen in the figures the sums are performing as expected. An interesting, and expected outcome is that in Figure~\ref{fig:p2debug} it takes 0.06 seconds to run $OMP$ and $No\ Reduc$ but in Figure~\ref{fig:p2} it takes 0.05 seconds. The $No\ OMP$ takes 0.40 seconds regardless. The reason for this behavior is that $OMP$ uses the cores you give it and at the time of recording the first figure the browser was open and running a video. When I recorded the second Figure I had closed my browser to maximize performance for multi-core processing. The $No\ OMP$ section of code was only running on one core, so it did not care that I had a video playing.


\newpage
\section{Problem 3}
Please see my \href{https://github.com/macattackftw/HighPerfComputing}{repository} for the full code breakdown for \href{https://github.com/macattackftw/HighPerfComputing/blob/master/HW1/problem3.cpp}{this problem}.
\subsection{Problem 3a}
\begin{minted}{c++}
    #pragma omp for
    for(i = 0; i < (long) sqrt(x); ++i) {
        a[i] = 2.3 * i;
        if (i < 10) 
            b[i] = a[i];
    }
\end{minted}

\subsection{Problem 3b}
This code section is not suitable for OpenMP because of the \&\& operator in comparison.
Per the \hyperlink{https://www.openmp.org/wp-content/uploads/openmp-4.5.pdf}{OpenMP documentation} \S2.6, p53:\\\\
\begin{tabular}{@{}ll}
$test$-$expr$ & One of the following:\\
& $var\ relational$-$op\ b$\\
& $b\ relational$-$op\ var$\\\\

$relational$-$op$ & One of the following:\\
& $<$  \\
& $<=$ \\
& $>$  \\
& $>=$ \\
\end{tabular}

\subsection{Problem 3c}
This code can be ran with $OMP$ but it is dependent on whether or not $foo()$ is threadsafe.
\begin{minted}{c++}
    #pragma omp for
    for(i = 0; i < n; ++i) {
        a[i] = foo(i);
    }
\end{minted}

\subsection{Problem 3d}
This problem is similar to 3c in that it is dependent on $foo()$. Nothing else is preventing this from being made parallel. Only comparisons and assignments are used. Access of $b[i]$ is fine because nothing is writing to it so there is no opportunity for trouble.
\begin{minted}{c++}
    #pragma omp for
    for(i = 0; i < n; ++i) {
        a[i] = foo(i);
        if(a[i] < b[i])
            a[i] = b[i];
    }
\end{minted}

\subsection{Problem 3e}
Cannot be ran in parallel because the $break$. 

\subsection{Problem 3f}
Standard use of $OMP$, though I feel that it didn't acutally use $OMP$. I tried to throw a reduction at it but I kept getting an error about $dotp$ being private. I threw a private local $sum$ into the for loop and added \textit{#pragma omp critical} followed by $dotp += sum$ but it gave no performance increase.
\begin{minted}{c++}
    dotp = 0;
    #pragma omp for
    for(i = 0; i < n; i++){
        dotp += a[i] * b[i];
    }
\end{minted}

\subsection{Problem 3g}
I thought this one was going to have trouble because of the $i\gets k$ portion but testing showed the values matched for the sequential sanity test and the $OMP$ portion. Of note though, $OMP$ was signficantly slower than sequential until I ramped up $k$.
\begin{minted}{c++}
    #pragma omp for
    for(i = k; i < 2 * k; ++i){
        a[i] = a[i] + a[i-k];
    }
\end{minted}

\subsection{Problem 3h}
This problem is the same as 3g except we are using what is essentially a shared constant, $b$ and we are running through the whole list, depending on what $k$ is. Other than that it behaves the same. Passed sanity check and ran without issue.
\begin{minted}{c++}
    #pragma omp for
    for(i = k; i < n; ++i){
        a[i] = b * a[i-k];
    }
\end{minted}


\newpage
\section{Problem 4}
\textit{Given a task that can be divided into m subtasks, each requiring one unit of time, how much time is needed for an m-stage pipeline to process n tasks?}\\

How many of the subtasks are dependent on other subtasks? If all $m$ subtasks require the previous subtask then $m * n$ time is needed to process n tasks. If they can all be ran independently and if we have enough threads we can run it in $n$ time. It is more likely that we will have some combination of subtasks that require other subtask completion, therefore the only blanket assumption that can be safely made is that it will take $m * n$ time.



\newpage
\section{Problem 5}
\textit{If the address of the nodes in a hypercube has n bits. How many nodes can it be at the most and how many edges does each node have? 
Give an algorithm that routes a message from node u to node v in this k-node hypercube in no more than log(k) steps.}\\

Hypercubes are generalized by dimensionality. A hypercube address containing $n$ bits will exist in $n$ dimensions ($d$). The maximum number of nodes is determined by $d$, and is defined as $2^d$. A trait of hypercubes is that each node will have $d$ edges. The total amount of edges would be defined as $d * 2^{d-1}$. The number of nodes you have to travel between $u$ and $v$ is equal to the number of differing bits.\\

For example:
\begin{itemize}
    \begin{tabular}{@{}llll}
    \item 000 & \implies & 111 & : 3 edges\\
    \item 001 & \implies & 100 & : 2 edges\\
    \item 110 & \implies & 011 & : 2 edges
    \end{tabular}
\end{itemize}

\noindent A traversal algorithm would be along the lines of Algorithm\ref{alg:1}\\
\begin{algorithm}[H]
\caption{Hypercube Traversal}
\label{alg:1}
\Statex Move from node $u$ to $v$
\State $i\gets 0$
\State $current\_bit\gets u[i]$
\State $left\_bit\gets u[i]$
\While {$i < dimension(hypercube)$}
    \State $current\_bit\gets u[i]$
    \If {current\_bit $=$ v[$i$]}
        \State $i\gets i+1$
    \Else
        \State $route(current\_bit)$ \Comment{Routes to neighbor node containing the correct bit}
    \EndIf
\EndWhile
\end{algorithm}

If built properly there will only be one node the algorithm can route to during the $route(current\_bit)$ section. Essentially we look at the left-most bit and if it is the same we move to the next bit, if it is different we must move to a new node. There will only be one node available because if they are on the same dimension they would have the same bit in that slot. For example on a 3-dimensional hypercube $000 \implies 100$ looks at the left bit and sees it is on a different dimension. There is only one node connected to 000 that follows the format 1xx, so that is the only place it can go. If we wanted to route from $000 \implies 010$ we would identify we are on the correct 3rd dimension, then look at the middle bit. From there we would see a need to move in the second dimension. There is only one connected node that matches in the second dimension, so that is where it would go.


\newpage
\section{Graduate Assignment}
\textit{Do a search on Shuffle-exchange network topology.  
Draw the network with 16 processor nodes (carefully numbering each node binary and showing what is a shuffle link, what is an exchange link).  If k is the number of digits in the binary address, how many nodes (n) are there?  With n nodes what is the diameter (d) of the networks, the bisection width (b) and how many edges/node?}\\

% I was 100% wrong, shuffle is not butterfly!
% When you first drew a shuffle-exchange network, also sometimes called a \href{https://en.wikipedia.org/wiki/Butterfly_network}{butterfly network}, I immediately thought that looked like a sorter I came up with in CSC300. I later learned that sort wasn't something I had uniquely developed and was in fact a \href{https://en.wikipedia.org/wiki/Bitonic_sorter}{bitonic sort}.\\

% If there are $k$ digits in the address then there will be a maximum of \textit{(k + 1) *} $2^k$ nodes. With $n$ nodes the diameter will be \textit{2 * k}. If the network is setup to wrap around then the diameter could be $k$. Bisection on butterfly networks is easy to see, visually, and is defined as \textit{k + 1}. Most nodes would contain \textit{4 edges}, where two come from ``below'' and two come from ``above''. The boundary nodes would contain \textit{2 edges}, ethier connecting ``up'' or ``down''.

\noindent There are a plethora of ways to make a shuffle-exchange network topology, such as:
\begin{itemize}
    \item Banyan
    \item Omega
    \item Indirect binary
\end{itemize}

\noindent In the \textit{omega} shuffle-exchange we can expect the following:
\begin{itemize}
    \item With $k$ bits in the address we can expect there to be $2^k$ input nodes $n$
    \item We can expect $n / 2 * log_2(n)$ shuffle-exchange nodes.
    \item Given the above, we expect a diameter $d$ to equal $k$.
    \item The bisection width is the number of inputs, $n$.
    \item There are \textit{4 edges} per node.
\end{itemize}

\noindent So for our example problem with 16 processor nodes:
\begin{itemize}
    \item We have $k = 4$, $n = 16$
    \item $16 / 2 * 4$, or 16 shuffle-exchange nodes
    \item A diameter $d = 4$
    \item Bisection width of 16
    \item \textit{4 edges} per shuffle-exchange node
\end{itemize}
\end{document}
